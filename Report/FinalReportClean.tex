\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}

% ---------- Packages ----------
\usepackage[margin=0.85in]{geometry} % Optimized margins
\usepackage{setspace}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{amsmath, amssymb}
\usepackage{siunitx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{xcolor}
\usepackage{titlesec}
\usepackage{float}

% ---------- Formatting ----------
\singlespacing
\setlength{\parskip}{0.5em}
\setlength{\parindent}{0pt}

% Fix for the "headheight too small" warning
\setlength{\headheight}{14pt}

\titleformat{\section}
  {\normalfont\Large\bfseries}{\thesection}{0.5em}{}
\titleformat{\subsection}
  {\normalfont\large\bfseries}{\thesubsection}{0.5em}{}
\titleformat{\subsubsection}
  {\normalfont\normalsize\bfseries}{\thesubsubsection}{0.5em}{}

\hypersetup{
  colorlinks=true,
  linkcolor=blue,
  urlcolor=blue,
  citecolor=black
}

% ---------- HEADERS ----------
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{Predicting Song Popularity}
\fancyhead[R]{\thepage}
\renewcommand{\headrulewidth}{0.4pt}

\begin{document}

% =========================================================
% TITLE PAGE
% =========================================================
\begin{titlepage}
    \centering
    \includegraphics[width=0.3\textwidth]{figures/image0}
    \vspace*{1cm}
    \rule{\linewidth}{0.5pt}
    {\LARGE\bfseries ANALYZING THE IMPACT OF AUDIO FEATURES ON SPOTIFY SONG POPULARITY\par}
    \rule{\linewidth}{0.5pt}
    \vspace{1cm}
    {\large\itshape A Machine Learning Approach to Predicting Song Popularity Based on Audio Attributes\par}
    \vspace{2cm}
    {\large\scshape A Project Report Presented in\\
    \large DATA 443 – Statistical Machine Learning\par}
    \vspace{1.5cm}
    {\large\scshape By\par}
    \vspace{0.3cm}
    {\Large Zoya Malik, Asjad Zubair, Yurii Bezborodov\\
    \large University of Calgary\par}
    \vspace{2cm}
    {\large Date: December 5, 2025\par}
    \vfill
    {\large\scshape Department of Mathematics \& Statistics\\
    University of Calgary\par}
\end{titlepage}

\newpage
\tableofcontents
\newpage

% =========================================================
% 1. INTRODUCTION
% =========================================================
\section{Introduction}

\subsection{Introduction and Objective}
Music is an art form that connects people around the world. Songs from different cultures, soundtracks from movies and games, classical music, orchestral music, love and passion for music exists globally and is evident when looking at streaming services and concerts. According to Spotify user statistics \cite{spotify_stats} the streaming service has 713 million monthly active users and is one of the most popular music streaming services available. This shows the importance of music in our lives. Of course, not all of the music available on Spotify is listened to equally. Only so many songs can be considered “popular” and the list of popular songs is constantly changing. Knowing this, we set out to determine whether the audio features of a song can be used to predict its popularity.

It is clear to us that there are many external factors that influence how popular a song becomes. The popularity of the artist themselves, marketing of the music, collaborating projects, etc. all have a strong influence on how many people will listen to a song. The objective of our project is not to predict what songs will go viral, but to examine whether measurable audio properties allow meaningful prediction of how popular a song will become. This project aims to quantify the relationship between audio features and song popularity using linear and non-linear models, and determine if predictions can be reasonably made.

\subsection{Dataset Description}
For this project we are using a dataset that contains over 80 thousand tracks spanning across 125 different genres \cite{kaggle_data}. Before any data cleaning or preprocessing, the raw dataset has 114,000 rows and 21 columns. These columns include audio features (e.g. danceability, loudness, acousticness, etc.), popularity (0-100), and categorical metadata (e.g. artists, genre, key, etc.). The dataset also contains duplicates, extreme outliers, and heavily skewed data.

\subsection{Research Question}
The primary focus of this project is to answer the question: \textbf{How accurately can we predict a song’s popularity using its audio attributes?} While we explore the answer to this question we will also be looking at which audio features are the strongest predictors of probability, and whether the strongest predictors differ across genres.

\subsection{Data Cleaning}
As mentioned earlier, the raw dataset has 114,000 rows. The obvious steps to clean the data and prepare it for exploratory data analysis is to check for missing values and duplicates as these add noise to our analysis. The dataset mentions that duplicate tracks can exist, but it is also possible for different songs to have the same name. To solve for this, duplicate tracks were detected using the track name, duration, and artists. After applying these filters, one row was dropped for having missing values, and over 30,000 rows were dropped for being duplicate tracks. Once this initial cleaning was done, the data was ready for initial exploratory analysis to help guide our preprocessing.

% =========================================================
% 3. EXPLORATORY DATA ANALYSIS
% =========================================================
\section{Exploratory Data Analysis}

\subsection{Summary Statistics and Correlations}
We first inspected the summary statistics (Figure~\ref{fig:summary-stats}) to check for reasonable ranges and identifying potential outliers. The correlation analysis (Figure~\ref{fig:corr-matrix}) revealed that no single feature strongly predicts popularity linearly; \textit{instrumentalness} had the strongest negative correlation, while \textit{loudness} showed a weak positive correlation.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/part4/4figure1}
    \caption{Summary statistics for numerical audio features.}
    \label{fig:summary-stats}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\linewidth]{figures/part4/4figure3}
    \caption{Correlation matrix heatmap showing weak linear relationships with popularity.}
    \label{fig:corr-matrix}
\end{figure}

\subsection{Distributions and Outliers}
Boxplots of the audio features (Figure~\ref{fig:boxplots}) highlight the presence of outliers, particularly in \textit{duration}, and the skewness of features like \textit{acousticness}. The popularity distribution itself (Figure~\ref{fig:pop-hist}) is heavily clustered between 20 and 60, with very few tracks achieving "hit" status ($>80$).

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/part4/4figure4}
    \caption{Boxplots of key audio features illustrating skew and outliers.}
    \label{fig:boxplots}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/part4/4figure5}
        \caption{Distribution of Popularity Scores.}
        \label{fig:pop-hist}
    \end{minipage}
    \hfill
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/part4/4figure6}
        \caption{Top Predictors vs. Popularity Tier.}
        \label{fig:top-predictors}
    \end{minipage}
\end{figure}

\subsection{Genre Effects and Interactions}
Genre plays a significant role in baseline popularity (Figure~\ref{fig:genre-bar}), with pop and metal scoring higher on average. Furthermore, we observed an interaction effect: the relationship between \textit{loudness} and popularity changes direction depending on the genre (Figure~\ref{fig:interaction}), motivating the use of interaction terms in our models.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/part4/4figure8}
    \caption{Top and Bottom 10 Genres by Average Popularity.}
    \label{fig:genre-bar}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\linewidth]{figures/part4/4figure7}
    \caption{Interaction: Loudness predicts popularity differently across genres (Acoustic vs Dance vs Metal).}
    \label{fig:interaction}
\end{figure}


\subsection{Preprocessing Steps}
As mentioned, our data analysis showed that the audio features were skewed to various degrees, but the popularity data was not heavily skewed. Due to this, we decided to bin the popularity data into tiers and let the models auto balance classes, allowing for a more stable target variable. We also found that there were tracks with extreme values for duration and unrealistic values for tempo. Filtering outliers that are shorter than 30 seconds or longer than 20 minutes, or tracks with a tempo of 0, would help normalize our data and allow us to stick to songs while avoiding tracks such as “rain noise”. Applying these filters led to an additional 228 rows of data being dropped. Now that our dataset contained “normalized” songs, we had to deal with skewed distributions to avoid our models training with bias. For features that were highly skewed (instrumentalness, acousticness, speechiness, liveness) the best way to achieve a normal distribution was to use a `PowerTransformer` with the Yeo Johnson method \cite{sklearn_pt} to approximate a normal distribution. Other features that are still bounded between 0 and 1 but are less skewed (danceability, energy, valence) could be normalized using the MinMaxScaler. These features do not possess an extreme enough skew to bias our models, but are skewed enough to add meaning to the data. Using a PowerTransformer here could potentially cause us to lose context for our models as the MinMaxScaler preserves shape, skew, and relative distances. Features that are unbounded (loudness, tempo, duration) were normalized using a StandardScaler to achieve normal distribution. Since “track\_genre”, “key”, and “time\_signature” are categorical features, we used OneHotEncoder to encode them which resulted in a significant increase in the number of binary features. Our exploratory analysis also showed that the relationship between “loudness” and “popularity” varied by genre which is something we want to explore further. However, machine learning models do not usually create or see these interactions automatically. Instead, we explicitly defined some notable interactions (loudness x acoustic, heavy metal, dance) which allows our machine learning models to learn the slopes of these interactions. The result of these preprocessing steps was a feature matrix with over 82,000 rows and 16 columns, ready to be split and used for training and validating machine learning models.



% =========================================================
% 4. MACHINE LEARNING MODELING
% =========================================================
\section{Machine Learning Modeling}
As our exploratory data analysis shows, this dataset has weak correlations and no clear linear structure in our plots. Given this we thought it would be best to use a logistic regression model, to see how a linear classifier handles non-linear data. To have a comparison, we decided to also use a support vector machine (SVM) with an RBF kernel. Logistic Regression assumes decision boundaries between classes are linear, is fast to train, and easy to interpret, making it an ideal baseline for our project. On the other hand, a SVM with an RBF kernel allows complex, curved decision boundaries. This model is more appropriate for our project given what our initial analysis revealed, and is expected to perform better than the logistic regression model. Using both models allows us to definitively determine whether there is linearity in the relationship between audio features and popularity. If there is not much linearity in these relationships, then comparing the two models will tell us whether flexible decision boundaries result in a significant increase in prediction accuracy. Our primary indicator of model performance is the macro-f1 score because it computes the f1 score per class and averages them, allowing all classes to be represented equally. This is critical for our project due to the number of songs in the higher popularity bins being noticeably less than the number of songs in other bins. Upon running our models the most immediate and noticeable difference was runtime. While the logistic regression model completed in under a minute, the SVM model took over 5 hours to complete. This is partly due to the fact that the RBF kernel computes similarity between every pair of training samples. Over 82,000 songs split into a test size of over 66,000 means the kernel has over 4 billion pairwise computations to make. Upon using GridSearch to identify the best parameters for each model, we found that for logistic regression the best parameters are \texttt{\{'model\_\_C': 0.1, 'model\_\_penalty': 'l2'\}}. The low C value means the model required stronger regularization, suggesting the data may be noisy, while the l2 penalty helps with generalization. The SVM model performed best with the parameters \texttt{\{'model\_\_C': 10.0, 'model\_\_gamma': 'scale'\}}. The high C value in this case means more complex boundaries, indicating that the data is not linearly separable and requires a more flexible boundary for increased performance.

% =========================================================
% 5. RESULTS AND INTERPRETATION
% =========================================================
\section{Results and Interpretation}

\subsection{Overall Performance}
The SVM with RBF kernel consistently outperformed the Logistic Regression baseline across all aggregate metrics (Table~\ref{tab:overall-metrics} and Figure~\ref{fig:overall-bar}). The improvement confirms the presence of non-linear boundaries, although the overall accuracy remains moderate ($\approx 62\%$).

\begin{table}[H]
\centering
\caption{Overall Performance Metrics}
\label{tab:overall-metrics}
\begin{tabular}{lcccc}
\toprule
Model & Accuracy & Macro F1 & Weighted F1 & Macro Recall \\
\midrule
Logistic Regression & 0.570 & 0.471 & 0.598 & 0.589 \\
SVM (RBF Kernel)    & 0.624 & 0.518 & 0.636 & 0.552 \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\linewidth]{figures/part5/5figure5}
    \caption{Overall performance comparison: SVM vs. Logistic Regression.}
    \label{fig:overall-bar}
\end{figure}

\subsection{Class-wise Analysis}
The confusion matrices (Figure~\ref{fig:confusion}) and class-wise F1 scores (Table~\ref{tab:class-f1}) reveal that both models struggle significantly with the highest popularity tier (80--100). While SVM improves performance in the mid-range tiers, the F1 score for the top tier remains low (0.166), indicating that audio features alone are insufficient to distinguish global hits.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\linewidth]{figures/part5/5figure6}
    \caption{Confusion Matrices. Note the high misclassification in the 80--100 column.}
    \label{fig:confusion}
\end{figure}

\begin{table}[H]
\centering
\caption{Class-wise F1 Scores}
\label{tab:class-f1}
\begin{tabular}{lcc}
\toprule
Popularity Tier & LogReg F1 & SVM F1 \\
\midrule
0--20   & 0.6587 & 0.6879 \\
20--40  & 0.6556 & 0.6919 \\
40--60  & 0.5841 & 0.6121 \\
60--80  & 0.3427 & 0.4332 \\
80--100 & 0.1154 & 0.1662 \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\linewidth]{figures/part5/5figure8}
    \caption{Class-wise F1 Score Trend.}
    \label{fig:class-trend}
\end{figure}


\subsection{Error Analysis}

To better understand where the models fail, we examined the misclassifications shown in the confusion matrices. 
Both models tend to make “near miss” errors, where songs are predicted into an adjacent popularity tier rather than an entirely unrelated one. 
This pattern indicates that the models are capturing a coarse structure in the data but struggle with finer distinctions.

A more important observation is that misclassification rates increase sharply for the 60--80 and 80--100 tiers. 
Songs in these ranges share similar audio profiles with mid-tier songs, making them difficult to separate based on acoustic data alone. 
Even the SVM, which can learn flexible nonlinear boundaries, fails to recover a distinct decision boundary for top-tier tracks. 
This aligns with the intuition that the highest popularity tier is governed more by external factors (artist fame, marketing, virality) than by measurable audio properties.

Overall, the error structure reinforces that audio features provide moderate predictive power for broad groupings, but not for distinguishing hit songs.

\subsection{Feature Importance Discussion}

Although SVMs do not provide traditional feature importances, we can examine Logistic Regression coefficients and EDA trends to understand which features contribute most to the predictions. 
Features such as loudness, energy, and danceability show weak but consistent positive associations with popularity, while instrumentalness displays a strong negative association. 
These trends align with the summary statistics and correlation heatmaps observed earlier.

However, the weak magnitude of all coefficients highlights the same conclusion reflected in model performance: no single audio feature exerts a dominant influence on popularity. 
Patterns are subtle, diffuse, and nonlinear, which explains why the SVM achieved higher performance than Logistic Regression.

\subsection{Model Limitations}

The overall performance of the models suggests inherent limitations in predicting popularity directly from audio features. 
Even with substantial preprocessing, normalization, and the use of nonlinear models, the predictive ceiling remains low for the highest popularity tier. 
This limitation arises from two factors: (1) the imbalance of the dataset, with far fewer hit songs than typical tracks, and (2) the absence of external metadata that strongly influences real-world popularity.

These limitations clarify that while audio-based models can detect general popularity structure, audio alone is insufficient to capture the complex mechanisms behind highly successful songs.

% =========================================================
% 6. FUTURE STEPS
% =========================================================
\section{Future Steps}
Based on our current findings, several ideas could further improve our ability to model and understand song popularity. First, we would extend the feature space by incorporating metadata such as artist popularity, release year, playlist placement, label information, and social/virality signals where possible (e.g., from charts or external APIs). This would allow us to explicitly test how much incremental predictive power comes from non-audio context compared to strictly acoustic features.

Also, future work could deepen our treatment of time and user behavior. Popularity is dynamic, so incorporating temporal features (e.g., weekly stream counts, release-to-peak time, decay curves) and experimenting with time-aware models could help capture rise-and-fall patterns that static snapshots miss. If feasible, we could also explore aggregate listener behavior (skips, saves, playlist adds) to better approximate engagement rather than relying solely on a single popularity score. Finally, we could prototype simple interfaces or dashboards, so the model isn’t just for analysis, but also helps people make decisions and gets better over time with real user feedback.

% =========================================================
% 7. CONCLUSION
% =========================================================
\section{Conclusion}
“Can audio features be used to accurately predict a song’s popularity, and which features contribute most?”
Our results show that audio features do contain useful information, especially for predicting broad popularity tiers. SVM, which can capture nonlinear patterns in the data, performed the best and were able to classify songs in the 0–60 popularity range with moderate accuracy. This suggests that combinations of basic audio characteristics (such as energy, danceability, and tempo) are meaningfully related to whether a song is generally unpopular, moderately popular, or somewhat successful.
However, our models struggled with the highest popularity tier (80–100). Even the best model could not reliably separate “hit” songs from more typical tracks. This indicates that audio features alone are not enough to explain extreme popularity. Factors such as artist fame, marketing, social media trends, and playlist placement likely play a much larger role when it comes to breakout hits.
Overall, we can say that audio-only models are helpful for coarse-grained prediction and for understanding broad patterns, but they are limited when we care about the very top of the charts. To better model top-tier success, future work should include extra metadata (artist, release context, exposure signals) and possibly multimodal approaches that combine audio with external information.

% =========================================================
% REFERENCES
% =========================================================
\phantomsection
\addcontentsline{toc}{section}{References}
\begin{thebibliography}{9}

\bibitem{spotify_stats}
Backlinko Team. 2025. \textit{Spotify User Stats (Updated September 2025)}. Backlinko. Retrieved December 1, 2025 from 
\url{https://backlinko.com/spotify-users}

\bibitem{kaggle_data}
MaharshiPandya. 2022. \textit{Spotify Tracks Dataset}. Kaggle. Retrieved December 1, 2025 from 
\url{https://www.kaggle.com/datasets/maharshipandya/-spotify-tracks-dataset/data}

\bibitem{sklearn_pt}
\textit{PowerTransformer — scikit-learn 1.7.2 documentation}. scikit-learn. Retrieved November 27, 2025 from 
\url{https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PowerTransformer.html}

\end{thebibliography}

\end{document}
